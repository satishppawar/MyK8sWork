# Day Five - Install Rook

---

In this section we will install [_Rook_](https://rook.io).  _Rook_ provides file, block and object storage for a _Kubernetes_ cluster.  Intially this was done using just [_Ceph_](https://ceph.com) but recently _Rook_ has started to incorporate other storage solutions as well.  We will focus on _Rooks_ use of _Ceph_.

---

## Overview

_Rook_ makes use of a _Kubernetes Operator_ to manage the provisioning, configuration and maintenance its components.  A _Kubernetes Operator_ is a method of packaging, deploying and managing _Kubernetes_ applications.  Discusion of _Kubernetes Operators_ is out of scope for this course but a very good overview can be found [here](https://coreos.com/operators/).

A high level overview and detailed information for _Rook_ can be found [here](https://rook.io/docs/rook/v0.8/).

The following diagram depicts the _Rook_ architecture.

<img src="images/rook-architecture.png" width="800px" />

From the preceeding diagram we can see that applications, contained within _Pods_, can use _PVCs_ (_PersistentVolumeClaims_) to request storage from the _Ceph_ components that are managed by _Rook_.  The _Ceph_ components are coloured red and the _Rook_ components are coloured blue.

We won't go into further detail about the _Ceph_ components.  You can find detailed information [here](http://docs.ceph.com/docs/master/).


## Install

The Quickstart guide can be found [here](https://rook.io/docs/rook/v0.8/ceph-quickstart.html).  The following steps replicate and extend the setps from this Quickstart.

```console
# Clone the Rook repository
git clone https://github.com/rook/rook

# Create the Rook Operator
cd rook/cluster/examples/kubernetes/ceph
kubectl create -f operator.yaml
```

This will create a `rook-ceph-system` _Namespace_ and deploy into it the _Rook Operator_ and other associated _Rook_ components.

```console
kubectl -n rook-ceph-system get pod -w -o wide
```

```console
NAME                                  READY     STATUS              RESTARTS   AGE
rook-ceph-agent-bm4wg                 1/1       Running   0          8m        192.168.26.13   node3
rook-ceph-agent-cgsp2                 1/1       Running   0          8m        192.168.26.11   node1
rook-ceph-agent-gfk7b                 1/1       Running   0          8m        192.168.26.12   node2
rook-ceph-operator-78d498c68c-c6snx   1/1       Running   0          11m       10.244.2.13     node2
rook-discover-9czw2                   1/1       Running   0          8m        10.244.1.10     node1
rook-discover-cdw2c                   1/1       Running   0          8m        10.244.3.8      node3
rook-discover-stkpn                   1/1       Running   0          8m        10.244.2.14     node2
```

**Note**

The _Docker_ images for the _Ceph_ components are quite large and will take som time to pull.

From the preceeding `kubectl` output we can see that as well as a single `rook-ceph-operator` _Pod_ there is also a `rook-ceph-agent` and a `rook-discover` _Pod_ deployed onto each worker node in the cluster.

Now that we have the base _Rook_ components deplyed we will setup the _Ceph_ storage cluster.

```console
kubectl create -f cluster.yaml
```

This will create a `rook-ceph` _Namespace_ and deploy into it the _Ceph_ components.

```console
kubectl -n rook-ceph get pod -w -o wide
```

```console
NAME                                  READY     STATUS      RESTARTS   AGE       IP            NODE
rook-ceph-mgr-a-9c44495df-rsqtg       1/1       Running     0          37s       10.244.3.10   node3
rook-ceph-mon0-hq4rr                  1/1       Running     0          1m        10.244.1.11   node1
rook-ceph-mon1-kc9kw                  1/1       Running     0          54s       10.244.2.15   node2
rook-ceph-mon2-h5bdj                  1/1       Running     0          46s       10.244.3.9    node3
rook-ceph-osd-id-0-74df7d44b9-7q66q   1/1       Running     0          27s       10.244.3.12   node3
rook-ceph-osd-id-1-76cd674fd8-lwch4   1/1       Running     0          26s       10.244.2.17   node2
rook-ceph-osd-id-2-67746ddc47-bvl55   1/1       Running     0          26s       10.244.1.13   node1
rook-ceph-osd-prepare-node1-lm4bn     0/1       Completed   0          32s       10.244.1.12   node1
rook-ceph-osd-prepare-node2-g67cx     0/1       Completed   0          32s       10.244.2.16   node2
rook-ceph-osd-prepare-node3-hxkd5     0/1       Completed   0          32s       10.244.3.11   node3
```

Now that we have the _Ceph_ components running we can create a _StorageClass_ resources to make _Block_ storage available for our _Pods_ to consume.

Information about how to setup _Block_ storage can be found [here](https://rook.io/docs/rook/v0.8/block.html).  It is summarised below.

```console
vi ~/tmp/storageclass.yaml
```

Add the following content to the `storageclass.yaml` file.

```yaml
apiVersion: ceph.rook.io/v1beta1
kind: Pool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 3
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-ceph-block
provisioner: ceph.rook.io/block
parameters:
  pool: replicapool
  #The value of "clusterNamespace" MUST be the same as the one in which your rook cluster exist
  clusterNamespace: rook-ceph
```

This `yaml` file defines a _Ceph_ storage cluster that maintains 3 copies of each piece of user data.  A highly resiliant solution.  And, the `yaml` file also defines the _StorageClass_.  So, let's go ahead and create these resources.

```console
kubectl create -f ~/tmp/storageclass.yaml
```

We can now see that the _StorageClass_ is available to be used.

```console
kubectl get storageclass
```

```console
NAME              PROVISIONER          AGE
rook-ceph-block   ceph.rook.io/block   24s
```

The _Block_ storage setup [page](https://rook.io/docs/rook/v0.8/block.html) also describes how to deploy a sample workload that consumes the _Block_ storage.  This is summarised below.

First, we'll create the workloads that request the storage.

```console
cd ..
kubectl -n default create -f mysql.yaml
kubectl -n default create -f wordpress.yaml
```

Now we can check that the _PVCs_ have been created.


```console
kubectl -n default get pvc
```

```console
NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
mysql-pv-claim   Bound     pvc-5ba0f080-9954-11e8-af3a-080027d53cd3   20Gi       RWO            rook-ceph-block   12h
wp-pv-claim      Bound     pvc-5cdb5ab4-9954-11e8-af3a-080027d53cd3   20Gi       RWO            rook-ceph-block   12h
```

And we can check the _PVs_ (_PersistentVolumes_) that were created to fulfil these _PVCs_.

```console
kubectl get pv
```

```console
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                      STORAGECLASS      REASON    AGE
pvc-5ba0f080-9954-11e8-af3a-080027d53cd3   20Gi       RWO            Delete           Bound     rook-ceph/mysql-pv-claim   rook-ceph-block             12h
pvc-5cdb5ab4-9954-11e8-af3a-080027d53cd3   20Gi       RWO            Delete           Bound     rook-ceph/wp-pv-claim      rook-ceph-block             12h
```

We will leave these workloads running so that in the next section we can see how the _Block_ storage is provisioned inside _Ceph_.


What we have done in this section is briefly explore the _Rook_ architecture, then we have installed _Rook_ into our cluster and finally we have deployed a sample workload to test that _Block_ storage can be provisioned.


# Next

In the next section we will do a quick tour of the _Ceph_ components that have been deployed by _Rook_.

[Next](05-04.md)